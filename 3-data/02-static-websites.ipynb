{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aceabf8",
   "metadata": {},
   "source": [
    "# Scraping data from static websites\n",
    "\n",
    "In this notebook, we will scrape content from static websites.\n",
    "\n",
    "Per default `pd.read_html` tries to use the lxml package. It can be installed with `conda install -c conda-forge lxml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d3c39",
   "metadata": {},
   "source": [
    "### 1. Scraping tables with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64f3234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9e7206",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_population = \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d473c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(url_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dfdcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf754d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target table (requires some searching)\n",
    "df_population = tables[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989629f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5778fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "df_population[\"City\"] = df_population[\"City\"].str.replace(\"\\[.*\\]\", \"\", regex=True)\n",
    "df_population = df_population[[\"2021rank\", \"City\", \"2021estimate\"]]\n",
    "df_population = df_population.rename(columns={\"2021rank\": \"Rank\",\n",
    "                                              \"2021estimate\": \"Population\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02de660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4461b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population.plot(x=\"Rank\", y=\"Population\", kind=\"scatter\",\n",
    "                   figsize=(10,7), color=\"red\", s=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3877846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points almost on straight line on a log scale here\n",
    "df_population.plot(x=\"Rank\", y=\"Population\", logx=True, logy=True, kind=\"scatter\",\n",
    "                   figsize=(10,7), color=\"red\", s=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595e696",
   "metadata": {},
   "source": [
    "What we see is an example of [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law). These [power laws](https://en.wikipedia.org/wiki/Power_law) are surprisingly general. For example, have a look how the first 10 million words in 30 Wikipedias: https://en.wikipedia.org/wiki/Zipf%27s_law#/media/File:Zipf_30wiki_en_labels.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac473e2e",
   "metadata": {},
   "source": [
    "### 2. Scraping with requests and beautiful soup + CSS selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb3f38",
   "metadata": {},
   "source": [
    "First, we will scrape the same table again but now select it directly with its associated CSS selector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b07d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_population = requests.get(url_population)\n",
    "soup = BeautifulSoup(page_population.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10039ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_selected = pd.read_html(str(soup.select(\"table.wikitable:nth-child(21)\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one table was collecte now\n",
    "len(tables_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poluation_alternative = tables_selected[0]\n",
    "df_poluation_alternative.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fefea",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "As an example how we can scrape less structured data on a static website, let us collect the first news article on the English page of the [University of Muenster](https://www.uni-muenster.de/en/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4475dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_wwu = \"https://www.uni-muenster.de/en/\"\n",
    "page = requests.get(url_wwu)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select(\"article.module:nth-child(2)\")[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f69f9",
   "metadata": {},
   "source": [
    "### 3. Scraping with requests and beautiful soup + regular expressions\n",
    "\n",
    "We can also sometimes scrape directly fom the HTML content with regular expressions without any need for CSS selectors. Let us try to obtain all course codes from the LSE's graduate course [catalogue](https://www.lse.ac.uk/resources/calendar/courseGuides/graduate.htm) as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ecbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_lse = \"https://www.lse.ac.uk/resources/calendar/courseGuides/graduate.htm\"\n",
    "page_lse = requests.get(url_lse)\n",
    "soup_lse = BeautifulSoup(page_lse.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_codes = re.findall(\"[A-Z]{2}\\d{1}\\w*\", soup_lse.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5eed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(course_codes[:5])\n",
    "print(len(course_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586291c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c59c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
