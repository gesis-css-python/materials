{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### GESIS Fall Seminar in Computational Social Science 2022\n",
    "### Introduction to Computational Social Science with Python\n",
    "# Day 3-2: Scraping Web Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "* Inspecting webpages\n",
    "* Parsing static HTML with BeautifulSoup\n",
    "* Scraping dynamic pages with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting webpages\n",
    "* Scraping is a very useful tool for accessing and analysing the vast amount of unstructured data on the web.\n",
    "* Much more efficient than bulk manual copying and pasting information you find on the web.\n",
    "* We need a systematic way of identifying and extracting the information we require from webpages.\n",
    "* To do this, we need to be able to read and understand the underlying HTML (HyperText Markup Language) and CSS (Cascading Style Sheets) that builds a webpage.\n",
    "* This is a brief introduction to scraping, much more detail in next week's GESIS course \"Automated Web Data Collection with Python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the file 'simple.html' for a simple webpage. Open it with both a web browser and a text editor and study its structure.\n",
    "\n",
    "Real pages are usually much more complicated!\n",
    "\n",
    "![html](figs/html.png \"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the file 'css_demo.html' for a simple webpage. Open it with both a web browser and a text editor and study its structure.\n",
    "\n",
    "We have added some style information with CSS. The CSS style information is often very useful for finding and selecting elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "* BeautifulSoup is a package for parsing html.\n",
    "* It creates a navigable tree of the structured html.\n",
    "* We can also easily search through the tree for the elements we require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('data/simple.html', 'r') as f:\n",
    "    webpage = f.read()\n",
    "\n",
    "soup = BeautifulSoup(webpage)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's navigate through the tree:\n",
    "\n",
    "print(soup.head)\n",
    "# print(soup.head.title)\n",
    "# print(soup.head.title.text)\n",
    "\n",
    "# print(soup.body.p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the css_demo.html file\n",
    "\n",
    "with open('data/css_demo.html', 'r') as f:\n",
    "    webpage = f.read()\n",
    "\n",
    "soup = BeautifulSoup(webpage)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use select() to find all the elements that match a particular tag\n",
    "\n",
    "print(soup.select('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or to find all the elements that match a particular tag and 'class'\n",
    "\n",
    "print(soup.select('.division-two'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è PRACTICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: \n",
    "# a) Use BeautifulSoup to navigate the css_demo.html file down to the bolded text \"formatted\"\n",
    "#    i.e., soup.body...\n",
    "# a) Use BeautifulSoup select() to find the same text\n",
    "#    i.e., soup.body...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: Get the text of all paragraphs in css_demo.html with the class \"text-about-web-scraping\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Open the file PL_table.html in a text editor. Search the html until you find the Premier League table.\n",
    "# Read the file into BeautifulSoup\n",
    "# Try to identify the selector that gives you all the table cell names (club names) from the table.\n",
    "# (Yes, this is meant to be quite difficult at this point!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing static HTML with BeautifulSoup\n",
    "* As you have just seen, real webpages are a lot more complicated than our toy examples!\n",
    "* We need a better way of identifying the element selection codes.\n",
    "* Let's navigate to Wikipedia's [List of United States cities by population](https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"Right-click > Inspect\" quickly shows us the HTML structure\n",
    "* [SelectorGadget](https://selectorgadget.com/) is a useful tool to instantly show us the relevant tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's integrate this into Python...\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url_population = \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
    "\n",
    "page_population = requests.get(url_population) # Requests gets the page html for us\n",
    "soup = BeautifulSoup(page_population.content) # Parse with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can extract the top 10 cities from the map as identified:\n",
    "\n",
    "soup.select(\".noresize b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è PRACTICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Re-open PL_table.html from Q3 in your web browser.\n",
    "# Use the Selector Gadget and BeautifulSoup to select the elements (club names) as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: Select all of the city names, as well as their associated links (.get('href')) from the map\n",
    "# at the bottom of the Wikipedia article List of United States cities by population\n",
    "# Save the result as a dictionary, with city name as the key, and full wikipedia link as the value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6: Loop through all of the links you have just collected.\n",
    "# For each of these city pages, extract all hyperlinks to pages within Wikipedia\n",
    "# (i.e. starting with /wiki/) from the main article content\n",
    "# Add time.sleep(1) as the last stage in your loop, don't overload the site!\n",
    "# Save the result as a dictionary with city name as key, and list of links as values\n",
    "# Perfect parsing here is quite tough!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping dynamic pages with Selenium\n",
    "* Not all of the web is simple HTML files.\n",
    "* Lots of dynamic content (e.g. Javascript) can only be accessed by having a proper web browser rendering the website\n",
    "* We may also want to interact with pages (e.g. scrolling, clicking) and even send data to websites.\n",
    "* **[Selenium](https://selenium-python.readthedocs.io/)** is the solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "* Ensure you [install Selenium](https://selenium-python.readthedocs.io/installation.html) correctly\n",
    " - You need to install the Python package `conda install selenium`.\n",
    " - You also need to download a 'driver' (Firefox recommended) for your system, and move it to the correct location on your system.\n",
    "* The 'driver' is an automated web browser that you can see performing the actions you program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this cell runs, you have correctly installed Selenium!\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"http://www.python.org\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem = driver.find_element(By.NAME, \"q\") # find the search box\n",
    "elem.clear() # clear anything currently in the search box\n",
    "elem.send_keys(\"Guido\")  #¬†type the keys \"Guido\"\n",
    "elem.send_keys(Keys.RETURN) #¬†Send a return key to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() #¬†Quit the driver once finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's interact with a page further by playing Happy Birthday\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://en.wikipedia.org/wiki/Happy_Birthday_to_You\")\n",
    "elem = driver.find_element(By.CSS_SELECTOR, \":nth-child(9) .mw-tmh-play-icon\") # find the play button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem.click() #¬†click the element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit() #¬†Quit the driver once finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use Selenium to extract data from a page on Amazon deals\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://www.amazon.com/gp/goldbox/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get selector from SelectorGadget for the deal names and use the driver to select elements\n",
    "elems = driver.find_elements(By.CSS_SELECTOR, \"#slot-15 .DealContent-module__truncate_sWbxETx42ZPStTc9jwySW\")\n",
    "dealnames = [x.text for x in elems]\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dealnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è PRACTICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7: Use Selenium to navigate to this shared Google Doc\n",
    "# https://docs.google.com/document/d/1NKY4eeVtBjNNdx1cpS8VPiyazVCPnQRfMBTB4yHtk7I/edit?usp=sharing\n",
    "# Select the document using elem = driver.find_element_by_xpath('//body')\n",
    "# Write a message on a **new line** using send_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8: Use Selenium to navigate to the Amazon deals page https://www.amazon.com/gp/goldbox/\n",
    "# As demonstrated previously, extract the names of all the deals on the page\n",
    "#¬†Then, find the element to progress to the next page of results and click it\n",
    "# Extract the names of all the deals on this page, and repeat for the first 5 pages of results\n",
    "#¬†Sleep 1s between pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9: If you have finished the previous exercises, consider how you could improve your scraper from Q8.\n",
    "# Scrape deals by category, or get the hyperlink, price, use a custom search etc.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
